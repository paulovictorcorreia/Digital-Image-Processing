:numbered:
:author: Paulo Victor Queiroz Correia -paulocorreiaufrn@gmail.com
:data-uri:
:icons: font
:experimental:
:stem:
:source-highlighter: coderay
:toc: left
:doctype: book

:caution-caption: Cuidado
:important-caption: Importante
:note-caption: Nota
:tip-caption: Dica
:warning-caption: Aviso
:appendix-caption: Appendice
:listing-caption: Listagem
:table-caption: Table
:toc-title: Summary
:preface-title: Preface
:version-label: Version

= Digital Image Processing = 
Paulo Victor Queiroz Correia - paulocorreiaufrn@gmail.com [2018.1 - UFRN/DCA]

This page contains the exercises of DIP course lectured by Professor Agostinho Brito Jr, and by that we have an apresentation of the problems and how we solved them, including codes, input images and output images. Student: Paulo Victor Queiroz Correia. Student number: 20170009258.

== Initial Concepts
There were no exercises on this section, but we have to present the makefile for compiling the code and the various modules of the OpenCV library:

[source,c++]
----
include::Makefile[]
----

For compiling a single c++ file, we must execute the following command for a filename.cpp file:
----
$ make filename 
----
== First Module
=== Manipulating pixels on an image
On this section, we're going to see how we can manipulate pixels on OpenCV.

==== Regions

This exercise asks us to make a program in which asks the user for 2 points on a two dimentional plane, P1 and P2, and then draw a rectangle with the inverted colors of the pixels in the area of the rectangle.

The base image we used is:

.Base image for the exercises on section 2
image::ManipulatingPixels/biel.png[]

For this first exercise, we wrote the following code:
[source, c++]
----
include::ManipulatingPixels/regions.cpp[]
----

The entry command on the Linux terminal was this:

.Command line entry
image::ManipulatingPixels/terminal_regions.png[]

And, finally, the output picture of the inverted colors on _biel.png_ picture:

.Inverted region of the picture
image::ManipulatingPixels/biel_regions.png[Regions]

==== Inverted Quadrants

For this exercise, we had the image as it shows on the scheme:

.Quadrants
image::ManipulatingPixels/quadrants.png[]

And then invert the quadrants in such way tha the image ends up like this:

.Inverted Quadrants
image::ManipulatingPixels/inverted_quadrants.png[]


To complete this task, we wrote the following code:

[source, c++]
----
include::ManipulatingPixels/trocaregioes.cpp[]
----


In which we first change the picture's pixel horizontally, using a nested loop for that, and then we changed vertically, using another nested loop. Also, we had to use
an auxiliar variable 'aux', so we could change the pixels without the need of another Mat object.

The output image we obtained using _biel.png_ was:

.Inverted biel.png
image::ManipulatingPixels/biel_inverted.png[]

=== Filling Regions

In this section, we were introduced to labelling techniques using OpenCV function floodFill, in which we used to count the objects in the following image _bolhas.png_:

.Base image for this section
image::BubblesCount/bolhas.png[]

==== Enhancing Algorithm
A good question to make is: what if there were more than 255 objects on the scene? To solve this problem, we could use an auxiliar variable, which would accumulate every time the number of objects reach 255, and the variable we used to label would be written down to zero, and the total number of objects on the scene would be the sum of the auxiliar variable and the number in the variable that counts objects at the end of the algorithm.

==== Counting regions with holes
Now, we were challenged to write a program to count the number of objects with holes in _bolhas.png_. To do this, we had to first remove the objects that were touching the border, because we have no idea of their actual shape. After, we painted the whole background with the color 1 in the greyscale, so we could diferentiate the holes from the background, making our work a lot more easier. After this we count the total amount of images on the scene.

The algorithm will read each pixel and check 3 things:

* If the current pixel is 0 and the previous has the color of the current label, confirming that we have a object with a hole, and then we paint the color of his corresponding label;
* If the current pixel is 0 and the previous has a color smaller the the current label, confirming that we are in another hole from the object, and then we paint the color of his corresponding label, count +1 bubble and  +1 label;
* If we reached the bottom right corner of the image and the label is smaller then the number of objects, so we write _i_ and _j_ down to zero and increases the label.

The code written for this problem is:

[source, c++]
----
include::BubblesCount/labelling2.cpp[]
----


The output image we obtained was:

.Output image of the program
image::BubblesCount/labeling.png[]

And the output on terminal:
----
Total number of objects: 21
Total number of objects with bubbles 9
----

=== Histograms

In this section, we are going to use the concepts of histogram for 2 things: calculate the equalization of a image stream and for constructing a primitive motion sensor, using a Playstation Eye, from Sony, for this task, and comparing the histograms of the last two images captured.

==== Equalization

The first exercize asked us to build an automatic equalizer using a camera and the images' histogram. The equalization was done for the three colors: red, green and blue. But before, what exactly means to equalize a picture?

To equalize a picture means to redistribute the colors stastistically along all the histogram range, adjusting image intensity to enhance contrast. And for doing this, we're going to use the following code:

[source, c++]
----
include::Histograms/equalize.cpp[]
----

In this code, we first create 3 different histograms with 64 different sets of colors, which means that the first element of the histogram will count pixels from 0 to 3, the second 4 to 7 and so on. After that, we capture an image from the Playstation Eye, split the image in three planes, one for each color, and use OpenCV function _equalizeHist()_ to equalize each one of them. Thus, we calculate the histogram for each plane and normalize them. Then we show the equalized picture side by side with the original picture and compare the results:

.Original picture
image::Histograms/image.png[]

.Equalized picture
image::Histograms/equalized.png[]


As we can see, we notice that the equalized picture is brightier than the original, thus we can notice that near the door we can notice quite easier that 2 rectangles of light coming from the street, as if the contrast of the image was enhanced.

==== Motion Detector
In this exercise, we were tasked to do a motion sensor with image processing, and to accomplish this task, we ought to analyse and process the image histogram, by comparing two following images histogram by correlation between them two. The correlation is a mathematical process that compares two signals(doesn't matter if sound, histogram or something else) and gives us the similarity between both of them, which means that if the correlation is 1, we are comparing a histogram with himself.

After giving a mathematical base, let's see how we would do in OpenCV. It is important to notice that we are capturing a colored image, and because of this, we have three different histograms for three diffrerent colors, that is a huge deal. It is very important to decide which color you want to compare, in order to have an efficient motion detector, because if we choose red, for example, it would be very sensitive, because most of our skin have a strong presence of red, so, to counter measure this, I chose the green histogram, because on the scene I was testing, the green was not vey present, meaning that it would not be as sensitive as the red histogram.

The code we used to build this histogram is the following:
[source, c++]
----
include::Histograms/motiondetector.cpp[]
----

In order to compare two different histograms, we capture two frames, then we calculate the green histogram, normalize it and finally compare two histograms with the following parameters:
[source, c++]
----
diff = compareHist(histR2, histR, 0);
----

In which each one of them means: the first two are the histograms we want to compare and the third is the kind of operation we want to use to compare them, and, as I said earlier, it is the correlation between them.
The function we use to calculate the histogram is this:


[source, c++]
----
calcHist(&planes[1], 1, 0, Mat(), histR2, 1,
             &nbins, &histrange,
             uniform, acummulate);//Calculate the second image histogram
----

The minimum similarity of the two histograms we accepted was 0.9925, valor that was decided after various tests with the Playstation Eye, which means that it could be different for other cameras because of their sensors.

=== Spacial Filtering

In this section we were first introduced to the use of spacial filters, such as the mean operation, laplacian, gaussian, horizontal and vertical borders and so on.

==== Laplacian of gaussian

The only exercise of this section was to build a laplacian of gaussian filter, which means apply a laplacian filter over a gaussian bluried image. To do this, we cas use two different kernels and obtain similar results. But, before this, we have to explaing what exactly is going on.

A gaussian distribution is something like this:

image::SpacialFiltteringI/equations/gauss.svg[]

And the laplacian is calculated with this operation:

image::SpacialFiltteringI/equations/Laplaciano.gif[]

Font: http://academic.mu.edu/phys/matthysd/web226/Lab02.htm, accessed on April 1th 2018.


I used two kernels to solve this problem, in which both of them were approximations of the laplacian of gaussian operation described. The 5x5 kernel used was:

image::SpacialFiltteringI/equations/lapgauss5x5.gif[]

And the 9x9 kernel used for the activity was:

image::SpacialFiltteringI/equations/lapgauss9x9.gif[]

And the code used to complete this task was:

[source, c++]
----
include::SpacialFiltteringI/spacialfiltter.cpp[]
----

The 5x5 laplacian of gaussian filter result was:

.5x5 kernel image result
image::SpacialFiltteringI/results/filter5x5.png[]

And the 9x9 laplacian of gaussian filter result was:

.9x9 kernel image result
image::SpacialFiltteringI/results/filter9x9.png[]



== Second Module

=== Pointilhism with Canny

On this section, we studied how canny lines' algorithm works and also had a look on pointilhism art. The task for this section is to improve the pointilhism algorithm given by the Professor, and choose one of the following methods to improve the pointilhism art:

* Draw big circles on a basic pointilhism image (the basic algorithm was given by the Professor Agostinho);
* Use the borders identified by the canny algorithm to draw on the respectively points of the generated image;
* Increasing Canny algorithm's threshold and, for each new pair generated, draw increasingly less small circles on each position found.

I chose the second one, due to it's practicity to implement and that we could easily input a trackbar to increase or decrease the threshold of the borders of the algorithm, so  it is seen on the following gif:

.Pointilhism using Canny lines
image::Pointilhism/pauio-pointilhism.gif[]



=== Kmeans

On this section, we studied how kmeans algorithm can be affected by a random initialization of the centers of each cluster, which causes, for each round of the algorithm run, different clusters, meaning that the clusters will have different centers, thus each photo will be different.

The code used for theses teste was the following:

[source, c++]
----
include::kmeans/kmeans-exerc.cpp[]
----

To do this exercise, we had to execute a loop to execute the kmeans algorithm 10 times, because if we executed
it in 10 different executed, the kmeans algorithm would always converge to the same center, although, by executing only one executable with one loop ten times, we could generate different centers randomly.

The hyphotesis for the fact that we could not generate 10 different images by executing 10 different times the 
executable is due to the OpenCV package. This part of the package may not implement a NULL seed, so it will 
always generate the same centers for 20 clusters with one round of executing the algorithm.

Also, we determined that it would quantify in 20 clusters, so that the clustered images will have 20 different colors.

The results are the following images:

.Result 1
image::kmeans/teste0.jpg[]

.Result 2
image::kmeans/teste1.jpg[]

.Result 3
image::kmeans/teste2.jpg[]

.Result 4
image::kmeans/teste3.jpg[]

.Result 5
image::kmeans/teste4.jpg[]

.Result 6
image::kmeans/teste5.jpg[]

.Result 7
image::kmeans/teste6.jpg[]

.Result 8
image::kmeans/teste7.jpg[]

.Result 9
image::kmeans/teste8.jpg[]

.Result 10
image::kmeans/teste9.jpg[]

As we can see, the bottom left of the picture was the most affected one, due to the proximity of the colors blue and purple when clusering.